{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Dataset\n",
    "This notebook trains a classifier on the Adult dataset using tensorflow and computes influence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 epochs will take ages on CPU, reduce or use GPU\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, test and validation datasets\n",
    "train_df = pd.read_csv(\"data/train-one-hot.csv\")\n",
    "train_target_df = train_df.pop('salary')\n",
    "\n",
    "test_df = pd.read_csv(\"data/test-one-hot.csv\")\n",
    "test_target_df = test_df.pop('salary')\n",
    "\n",
    "val_df = pd.read_csv(\"data/val-one-hot.csv\")\n",
    "val_target_df = val_df.pop('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 examples\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(63,)))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "checkpoint_path = \"cp_training_adult/cp-{epoch:04d}\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#BATCHES_PER_EPOCH = int(24129/BATCH_SIZE)\n",
    "\n",
    "# creating a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "model.summary()\n",
    "\n",
    "# save initialised model\n",
    "model.save(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "history = model.fit(train_df.values,\n",
    "                    train_target_df.values,\n",
    "                    validation_data=(val_df.values, val_target_df.values),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[cp_callback, tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=20)])\n",
    "end = time.time()\n",
    "print(\"Total time:\", datetime.timedelta(seconds=end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Summary of the model performance. The defined methods can be used to compare multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model plotter\n",
    "def plot_model_results(history, clr, i=\"_alt\"):\n",
    "    ax[0].plot(history.history[\"loss\"], \"{}\".format(clr), label=\"M{} Train loss\".format(i), linewidth=2)\n",
    "    ax[0].plot(history.history[\"val_loss\"], \"{}--\".format(clr), label=\"M{} Val loss\".format(i), linewidth=2)\n",
    "    ax[1].plot(history.history[\"accuracy\"], \"{}\".format(clr), label=\"M{} Train accuracy\".format(i), linewidth=1.5)\n",
    "    ax[1].plot(history.history[\"val_accuracy\"], \"{}--\".format(clr), label=\"M{} Val accuracy\".format(i), linewidth=1.5)\n",
    "    ax[0].set_xlabel(\"$Epochs$\", fontsize=16), ax[1].set_xlabel(\"$Epochs$\", fontsize=16)\n",
    "    ax[0].set_ylabel(\"$Loss$\", fontsize=16), ax[1].set_ylabel(\"$Accuracy$\", fontsize=16)\n",
    "    ax[0].set_title(\"Loss\", fontsize=18), ax[1].set_title(\"Accuracy\", fontsize=18)\n",
    "    ax[0].legend(frameon=False, fontsize=14), ax[1].legend(frameon=False, fontsize=14)\n",
    "    \n",
    "# print results\n",
    "def return_results(model, test_features, test_labels, i=\"0\"):\n",
    "    # Evaluate model comparison\n",
    "    loss, acc = model.evaluate(test_features, test_labels, verbose=0)\n",
    "    print(\"M{}|| Accuracy: {:.2f}% --- Loss: {:.2f}\".format(i, 100 * acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result summary \n",
    "return_results(model, test_df.values, test_target_df.values)\n",
    "\n",
    "# create a loss & accuracy subplot\n",
    "f, ax = plt.subplots(figsize=(14, 6), ncols=2)\n",
    "\n",
    "# plot results of each model\n",
    "plot_model_results(history, \"g\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Influence \n",
    "#### Incorprating the self-influence code outlined in [TracIn paper](https://github.com/frederick0329/TracIn/blob/master/imagenet/resnet50_imagenet_self_influence.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate self-influence of batch members.\n",
    "@tf.function\n",
    "def run_self_influence(features, labels, models):\n",
    "    self_influences = []\n",
    "    for m in models:\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(m.trainable_weights)\n",
    "            probs = m(features, training=False)\n",
    "            loss = tf.keras.losses.binary_crossentropy(tf.reshape(labels, shape=(-1,1)), probs, from_logits=True)\n",
    "        grads = tape.jacobian(loss, m.trainable_weights)\n",
    "        scores = tf.add_n([tf.math.reduce_sum(\n",
    "            grad * grad, axis=tf.range(1, tf.rank(grad), 1)) \n",
    "            for grad in grads])\n",
    "        self_influences.append(scores)  \n",
    "\n",
    "    # using probs from last checkpoint\n",
    "    probs, predicted_labels = tf.math.top_k(probs, k=1)\n",
    "    return tf.math.reduce_mean(tf.stack(self_influences, axis=-1), axis=-1), labels, probs, predicted_labels\n",
    "\n",
    "# method to concatenate all of the batch results together\n",
    "def memorisation_results(memorisation, features, labels, probs, predicted_labels):\n",
    "    result_dictionary = {\n",
    "        \"memorisation\": np.array(np.concatenate(memorisation)),\n",
    "        \"features\": np.concatenate(features),\n",
    "        \"labels\": np.concatenate(labels),\n",
    "        \"probs\": np.concatenate(probs),\n",
    "        \"predicted_labels\": np.concatenate(predicted_labels)\n",
    "    }\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method incorprates **run_self_influence** and **memorisation_results** to return results for any given model scenario. It is used to study the comparisons between different CP memorisation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_self_influence(train_ds, models):\n",
    "    ds_memorisation = []\n",
    "    ds_features = []\n",
    "    ds_labels = []\n",
    "    ds_probs = []\n",
    "    ds_predicted_labels = []\n",
    "\n",
    "    start = time.time()\n",
    "    for features, labels in train_ds:\n",
    "        memorisation_score, labels, probs, predictied_labels = run_self_influence(features, labels, models)\n",
    "        ds_memorisation.append(memorisation_score)\n",
    "        ds_features.append(features)\n",
    "        ds_labels.append(labels)\n",
    "        ds_probs.append(probs)\n",
    "        ds_predicted_labels.append(predictied_labels)\n",
    "    end = time.time()\n",
    "    print(\"Total time:\", datetime.timedelta(seconds=end - start))\n",
    "    \n",
    "    return memorisation_results(ds_memorisation, ds_features, ds_labels, ds_probs, ds_predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method to load the desired model weights of a single or a list of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_models(epochs):\n",
    "    loaded_models = []\n",
    "    for epoch in epochs:\n",
    "        path = \"{}/cp_training_adult/cp-00{:02d}\".format(os.getcwd(), epoch)\n",
    "        if os.path.exists(path) == False:\n",
    "            print (\"File not found: cp-00{:02d}\".format(epoch))\n",
    "        else:\n",
    "            model = tf.keras.models.load_model(\"cp_training_adult/cp-00{:02d}\".format(epoch))\n",
    "            loaded_models.append(model)\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that uses **batch_self_influence** to generate self-influence results for all the models loaded in *models*. \n",
    "- First result is for the zeroth epoch\n",
    "- The last result is an averaged result across all the epochs, except the zeroth epoch\n",
    "- Returns a list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(train_ds, models):\n",
    "    results = [] \n",
    "    if len(models) > 1:\n",
    "        for model in models:\n",
    "            results.append(batch_self_influence(train_ds, [model]))\n",
    "    results.append(batch_self_influence(train_ds, models[1:])) # ave. self-influence\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create influence-based ordered dataset\n",
    "Running the self influence method over the entire training dataset in batches. The output for memorisation score, labels and probabilities are stored in lists specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array to a tensor to divide into batches\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_df.values, train_target_df.values))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from SGD optimizer with fixed learning rate (0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model index is later used to label plots\n",
    "model_cps = [0, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "models = return_models(model_cps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# returns all model results in a list \n",
    "#  > zeroth index holds averaged CP\n",
    "results = get_results(train_ds, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results into a pickle file to analyse in **adult_analysis.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save options\n",
    "EXTENSION = \"scan_results_shuffled_1000\"\n",
    "\n",
    "# store data (serialize)\n",
    "with open('results/{}.pickle'.format(EXTENSION), 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

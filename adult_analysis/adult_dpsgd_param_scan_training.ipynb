{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult DP-SGD Training\n",
    "This notebook is used for training the adult dataset on dp-sgd algoriths using the tf-privacy libraries. The primary use case is in running experiments to determine optimal hyperparameters for data pruning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "from os import path\n",
    "import pickle\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tf-privacy libraries\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy_lib\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import VectorizedDPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import VectorizedDPKerasAdagradOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to remove random entries, rounding to a dataset divisible by 100\n",
    "def remove_random_entries(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df.drop(np.random.choice(df.index, (df.shape[0]%100), replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, test and validation datasets\n",
    "train_df = remove_random_entries(\"data/train-one-hot.csv\")\n",
    "train_target_df = train_df.pop('salary')\n",
    "\n",
    "test_df = remove_random_entries(\"data/test-one-hot.csv\")\n",
    "test_target_df = test_df.pop('salary')\n",
    "\n",
    "val_df = remove_random_entries(\"data/val-one-hot.csv\")\n",
    "val_target_df = val_df.pop('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NUM_TRAIN_EXAMPLES=len(train_target_df.values)\n",
    "NUM_TRAIN_EXAMPLES=len(train_target_df.values)\n",
    "EPOCHS=100\n",
    "BATCH_SIZE=100\n",
    "N_MICROBATCHES=100\n",
    "LEARNING_RATE=0.001\n",
    "#L2_NORM_CLIP=5.9\n",
    "#NOISE_MULTIPLIER=1.0\n",
    "DELTA=1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating epsilon\n",
    "A method which takes in steps, batch_size, num_training_examples and noise_multiplier to return privacy spent in steps taken, or equivilently, in epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method which takes in steps and returns privacy spent in steps taken\n",
    "# > STEPS\n",
    "# > NOISE_MULTIPLIER\n",
    "# > BATCH_SIZE\n",
    "# > DELTA\n",
    "def compute_epsilon(steps, batch_size, num_training_examples, noise_multiplier):\n",
    "    \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "    if noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "    sampling_probability = batch_size / num_training_examples\n",
    "    rdp = compute_rdp(q=sampling_probability,\n",
    "                      noise_multiplier=noise_multiplier,\n",
    "                      steps=steps,\n",
    "                      orders=orders)\n",
    "    # Delta is set to approximate 1 / (number of training points).\n",
    "    return get_privacy_spent(orders, rdp, target_delta=DELTA)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running dp-sgd\n",
    "Method which takes in a list of clipping norms and noise multipliers to scan over these parameters for a list of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method to scan over both clipping_norm and noise_multiplier and run a training loop for each pair. \n",
    "# returns a pandas df for the saved results\n",
    "def run_dpsgd_scan(l2_norm_clip_scan, noise_multiplier_scan, epoch_scan):\n",
    "    start = time.time()\n",
    "    \n",
    "    columns = ['epoch','noise_multiplier', 'clipping_norm', 'acc', 'val_acc', 'epsilon']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    data = []\n",
    "    \n",
    "    total_loops = len(l2_norm_clip_scan)*len(noise_multiplier_scan)\n",
    "    current_loop = 0\n",
    "    \n",
    "    for noise_multiplier in noise_multiplier_scan:\n",
    "        for l2_norm_clip in l2_norm_clip_scan:\n",
    "            # reset tf session\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            # redefining variables\n",
    "            L2_NORM_CLIP=l2_norm_clip\n",
    "            NOISE_MULTIPLIER=noise_multiplier\n",
    "\n",
    "            # set optimiser options\n",
    "            optimizer = VectorizedDPKerasSGDOptimizer(\n",
    "                l2_norm_clip=L2_NORM_CLIP,\n",
    "                noise_multiplier=NOISE_MULTIPLIER,\n",
    "                num_microbatches=N_MICROBATCHES,\n",
    "                learning_rate=LEARNING_RATE\n",
    "            )\n",
    "            # define model\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.Input(shape=(63,)),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(1)]\n",
    "            )\n",
    "            # compile model\n",
    "            model.compile(optimizer=optimizer,\n",
    "                          loss=tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                                  reduction=tf.losses.Reduction.NONE),\n",
    "                          metrics=['accuracy'])\n",
    "            # start training\n",
    "            history = model.fit(train_df.values,\n",
    "                                train_target_df.values,\n",
    "                                validation_data=(val_df.values, val_target_df.values),\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                epochs=EPOCHS, \n",
    "                                verbose=0)\n",
    "            # calculate and append information required: \n",
    "            for epoch in epoch_scan:\n",
    "                STEPS = epoch * NUM_TRAIN_EXAMPLES / BATCH_SIZE\n",
    "                values = [epoch, \n",
    "                          NOISE_MULTIPLIER, \n",
    "                          L2_NORM_CLIP, \n",
    "                          history.history[\"accuracy\"][epoch-1],\n",
    "                          history.history[\"val_accuracy\"][epoch-1],\n",
    "                          compute_epsilon(STEPS,\n",
    "                                          BATCH_SIZE,\n",
    "                                          NUM_TRAIN_EXAMPLES,\n",
    "                                          NOISE_MULTIPLIER)]\n",
    "                zipped = zip(columns, values)\n",
    "                a_dictionary = dict(zipped)\n",
    "                data.append(a_dictionary)\n",
    "                \n",
    "            # printing information loop information\n",
    "            loss, acc = model.evaluate(val_df.values, val_target_df.values, verbose=0)\n",
    "            current_loop += 1\n",
    "            print (\"# {} out of {} -- Noise: {} -- Clipping Norm: {} -- Accuracy {}\".format(current_loop,\n",
    "                                                                                            total_loops, \n",
    "                                                                                            NOISE_MULTIPLIER, \n",
    "                                                                                            L2_NORM_CLIP, \n",
    "                                                                                            acc))\n",
    "            print(\"Elapsed time:\", datetime.timedelta(seconds=time.time() - start))\n",
    "    end = datetime.timedelta(seconds=time.time() - start)\n",
    "    df = df.append(data, True)\n",
    "    print(\"Completed {} experiments in {}\".format(total_loops, end))\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping_norm = [2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.25, 6.5, 6.75, 7, 7.25, 7.5, 7.75, 8, 8.25, 8.5, 8.75, 9.0, 9.25, 9.5, 9.75, 10]\n",
    "noise_multiplier = [0.9, 1.0, 1.1, 1.15, 1.2, 1.25, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.15, 2.2, 2.25, 2.3, 2.4, 2.5]\n",
    "epochs = [20, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scan over all parameters and write results to a dataframe\n",
    "df = run_dpsgd_scan(clipping_norm, noise_multiplier, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df.to_csv('results/dp_sgd_scan_results_4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult DP-SGD Training\n",
    "This notebook is used to train pruned and non-pruned datasets by using pre-selected noise multiplier and clipping norm pairs. The script produces a pandas dataframe for accuracy, validation accuracy and epsilon for a list of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "from os import path\n",
    "import pickle\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tf-privacy libraries\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy_lib\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import VectorizedDPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import VectorizedDPKerasAdagradOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to remove random entries, rounding to a dataset divisible by 100\n",
    "def prune_random_entries(df, prune_list, batch_size):\n",
    "    new_df = df.drop(np.random.choice(df.index, len(prune_list), replace=False))\n",
    "    return new_df.drop(np.random.choice(new_df.index, (new_df.shape[0]%batch_size), replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to remove influential entries\n",
    "# > remainder 100 entries are removes randomly thereafter\n",
    "def prune_influential_entries(df, prune_list, batch_size):\n",
    "    new_df = df.drop(df.index[prune_list])\n",
    "    return new_df.drop(np.random.choice(new_df.index, (new_df.shape[0]%batch_size), replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method which takes in steps and returns privacy spent in steps taken\n",
    "# > STEPS\n",
    "# > NOISE_MULTIPLIER\n",
    "# > BATCH_SIZE\n",
    "# > DELTA\n",
    "def compute_epsilon(steps, batch_size, num_training_examples, noise_multiplier, delta):\n",
    "    \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "    if noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "    sampling_probability = batch_size / num_training_examples\n",
    "    rdp = compute_rdp(q=sampling_probability,\n",
    "                      noise_multiplier=noise_multiplier,\n",
    "                      steps=steps,\n",
    "                      orders=orders)\n",
    "    # Delta is set to approximate 1 / (number of training points).\n",
    "    return get_privacy_spent(orders, rdp, target_delta=delta)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, test and validation datasets\n",
    "train_df = pd.read_csv(\"data/train-one-hot.csv\")\n",
    "\n",
    "test_df = pd.read_csv(\"data/test-one-hot.csv\")\n",
    "test_target_df = test_df.pop('salary')\n",
    "\n",
    "val_df = pd.read_csv(\"data/val-one-hot.csv\")\n",
    "val_target_df = val_df.pop('salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon calculator analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "# integrate line plot \n",
    "def return_area(x_array, y_array):\n",
    "    return auc(x_array, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_training_examples = 21700\n",
    "epoch = 100\n",
    "\n",
    "steps = epoch * num_training_examples / batch_size\n",
    "print (steps)\n",
    "compute_epsilon(steps, batch_size, num_training_examples, NOISE_MULTIPLIER, DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA=1e-5\n",
    "BATCH_SIZE = [50, 100, 125, 150, 175, 200, 225, 250, 300]\n",
    "TRAINING_SIZE = [12000, 16800, 21700, 23600, 24129]\n",
    "epoch_scan = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "\n",
    "# creating a dataframe to store information\n",
    "columns = ['training_size', 'batch_size', 'epoch', 'batches', 'steps', 'sampling_prob', 'epsilon']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "data = []\n",
    "\n",
    "for training_size in TRAINING_SIZE:\n",
    "    for batch_size in BATCH_SIZE:\n",
    "        for epoch in epoch_scan:\n",
    "            STEPS = epoch * training_size / batch_size\n",
    "            SAMPLING_PROB = batch_size / training_size\n",
    "            BATCHES = STEPS*SAMPLING_PROB*batch_size\n",
    "            EPSILON = compute_epsilon(STEPS, batch_size, training_size, 2.5, DELTA)\n",
    "            values = [training_size, \n",
    "                      batch_size, \n",
    "                      epoch, \n",
    "                      BATCHES, \n",
    "                      STEPS, \n",
    "                      SAMPLING_PROB, \n",
    "                      EPSILON]\n",
    "            zipped = zip(columns, values)\n",
    "            a_dictionary = dict(zipped)\n",
    "            data.append(a_dictionary)\n",
    "df = df.append(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maker(df, training_size, batch_size, x_axis, y_axis):\n",
    "    \n",
    "    new_data = df.loc[(df['training_size'] == training_size) & (df['batch_size'] == batch_size)]\n",
    "    \n",
    "    x, y = new_data[x_axis[0]], new_data[y_axis], \n",
    "    ax[0, 0].plot(x, y, label=\"N: {:.0f}, Lot: {:.0f}\".format(training_size, batch_size), linewidth=2)\n",
    "    ax[0, 0].set(ylabel=y_axis, xlabel=x_axis[0])\n",
    "    \n",
    "    x, y = new_data[x_axis[1]], new_data[y_axis], \n",
    "    ax[0, 1].plot(x, y, label=\"N: {:.0f}, Lot: {:.0f}\".format(training_size, batch_size), linewidth=2)\n",
    "    ax[0, 1].set(ylabel=y_axis, xlabel=x_axis[1])\n",
    "    \n",
    "    x, y = new_data[x_axis[2]], new_data[y_axis], \n",
    "    ax[1, 0].plot(x, y, label=\"N: {:.0f}, Lot: {:.0f}\".format(training_size, batch_size), linewidth=2)\n",
    "    ax[1, 0].set(ylabel=y_axis, xlabel=x_axis[2])\n",
    "\n",
    "    x, y = new_data[x_axis[3]], new_data[y_axis], \n",
    "    ax[1, 1].plot(x, y, label=\"N: {:.0f}, Lot: {:.0f}\".format(training_size, batch_size), linewidth=2)\n",
    "    ax[1, 1].set(ylabel=y_axis, xlabel=x_axis[3])\n",
    "    \n",
    "    ax[0, 0].legend(), ax[0, 1].legend(), ax[1, 0].legend(), ax[1, 1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot options\n",
    "y_axis = 'epsilon'\n",
    "x_axis = ['epoch', 'steps', 'sampling_prob', 'batches']\n",
    "\n",
    "# create a loss & accuracy subplot\n",
    "# create a loss & accuracy subplot\n",
    "f, ax = plt.subplots(figsize=(15, 12), ncols=2, nrows=2)\n",
    "\n",
    "#for batch_size in BATCH_SIZE:\n",
    "#    for training_size in TRAINING_SIZE:\n",
    "#            plot_maker(df, training_size, batch_size, x_axis, y_axis)\n",
    "\n",
    "\n",
    "for batch_size in BATCH_SIZE:\n",
    "    plot_maker(df, 21700, batch_size, x_axis, y_axis)\n",
    "    \n",
    "\n",
    "plot_maker(df, 12000, 50, x_axis, y_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot options\n",
    "y_axis = 'epsilon'\n",
    "x_axis = ['epoch', 'steps', 'sampling_prob', 'batches']\n",
    "\n",
    "# create a loss & accuracy subplot\n",
    "# create a loss & accuracy subplot\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "\n",
    "for batch_size in BATCH_SIZE:\n",
    "    for training_size in TRAINING_SIZE:\n",
    "        new_data = df.loc[(df['training_size'] == training_size) & (df['batch_size'] == batch_size)]\n",
    "        x, y = new_data['steps'], new_data['epsilon']\n",
    "        ax.plot(x, y, label=\"N: {:.0f}, Lot: {:.0f}\".format(training_size, batch_size), linewidth=2)\n",
    "        ax.set(ylabel='epsilon', xlabel='steps')\n",
    "\n",
    "ax.legend(fontsize=10, bbox_to_anchor=(1.0, 1), loc='upper right', ncol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prune indices\n",
    "Loads a dictionary which stores a list of applied prune fractions (\"frac_list\") and the indices to be pruned (\"prune_indices\"). frac_list:[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pruned indices\n",
    "#> dictionary of \n",
    "with open('results/pruned_dataset_cpave_bs500_double_test.pickle', 'rb') as handle:\n",
    "    prune_indices_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_list = prune_indices_list['prune_indices'][4]\n",
    "prune_random_entries(train_df, prune_list, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_TRAIN_EXAMPLES=len(train_target_df.values)\n",
    "EPOCHS=100\n",
    "BATCH_SIZE=100\n",
    "N_MICROBATCHES=100\n",
    "LEARNING_RATE=0.001\n",
    "DELTA=1e-5\n",
    "NOISE_MULTIPLIER = 2.5\n",
    "L2_NORM_CLIP = 10\n",
    "epoch_scan = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "This functions allows a way to train multiple experiments over different parameter scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dpsgd_scan_batch_size(l2_norm_clip, \n",
    "                              noise_multiplier, \n",
    "                              epoch_scan, \n",
    "                              prune_type, \n",
    "                              prune_indices_list,\n",
    "                              BATCH_SIZE_ARRAY,\n",
    "                              EXPERIMENTS):\n",
    "    start = time.time()\n",
    "    total_loops = len(prune_indices_list[\"frac_list\"])*EXPERIMENTS*len(BATCH_SIZE_ARRAY)\n",
    "    current_loop = 0\n",
    "    \n",
    "    \n",
    "    # creating a dataframe to store information\n",
    "    columns = ['run_number', 'prune_type', 'prune_frac', 'batch_size', 'steps', 'epochs', 'noise_multiplier', 'clipping_norm', 'acc', 'val_acc', 'epsilon']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    data = []\n",
    "    \n",
    "    for i, prune_frac in enumerate(prune_indices_list[\"frac_list\"]):        \n",
    "        for batch_size in BATCH_SIZE_ARRAY:\n",
    "            \n",
    "            N_MICROBATCHES=batch_size\n",
    "            \n",
    "            # get influential prune list\n",
    "            prune_list = prune_indices_list[\"prune_indices\"][i]\n",
    "            \n",
    "            # prune dataset\n",
    "            if prune_type == \"random\":\n",
    "                pruned_train_df = prune_random_entries(train_df, prune_list, batch_size)\n",
    "                pruned_train_target_df = pruned_train_df.pop('salary')\n",
    "            else:\n",
    "                pruned_train_df = prune_influential_entries(train_df, prune_list, batch_size)\n",
    "                pruned_train_target_df = pruned_train_df.pop('salary')\n",
    "            \n",
    "            # total number of training examples\n",
    "            NUM_TRAIN_EXAMPLES = len(pruned_train_df.values)\n",
    "\n",
    "            for run in range(EXPERIMENTS):\n",
    "                # reset tf session\n",
    "                tf.keras.backend.clear_session()\n",
    "                # set optimiser options\n",
    "                optimizer = VectorizedDPKerasSGDOptimizer(\n",
    "                    l2_norm_clip=L2_NORM_CLIP,\n",
    "                    noise_multiplier=NOISE_MULTIPLIER,\n",
    "                    num_microbatches=N_MICROBATCHES,\n",
    "                    learning_rate=LEARNING_RATE\n",
    "                )\n",
    "                # define model\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.Input(shape=(63,)),\n",
    "                    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(1)]\n",
    "                )\n",
    "                # compile model\n",
    "                model.compile(optimizer=optimizer,\n",
    "                              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                                                      reduction=tf.losses.Reduction.NONE),\n",
    "                              metrics=['accuracy'])\n",
    "                # start training\n",
    "                history = model.fit(pruned_train_df.values,\n",
    "                                    pruned_train_target_df.values,\n",
    "                                    validation_data=(val_df.values, val_target_df.values),\n",
    "                                    batch_size=batch_size,\n",
    "                                    epochs=EPOCHS, \n",
    "                                    verbose=0)\n",
    "                # calculate and append information required: \n",
    "                for epoch in epoch_scan:\n",
    "                    STEPS = epoch * NUM_TRAIN_EXAMPLES / batch_size\n",
    "                    values = [run,\n",
    "                              \"none\" if prune_frac==0 else \"random\" if prune_type == \"random\" else \"influential\",\n",
    "                              prune_frac, \n",
    "                              batch_size, \n",
    "                              STEPS,\n",
    "                              epoch,\n",
    "                              NOISE_MULTIPLIER, \n",
    "                              L2_NORM_CLIP, \n",
    "                              history.history[\"accuracy\"][epoch-1],\n",
    "                              history.history[\"val_accuracy\"][epoch-1],\n",
    "                              compute_epsilon(STEPS,\n",
    "                                              batch_size,\n",
    "                                              NUM_TRAIN_EXAMPLES,\n",
    "                                              NOISE_MULTIPLIER, \n",
    "                                              DELTA)]\n",
    "                    zipped = zip(columns, values)\n",
    "                    a_dictionary = dict(zipped)\n",
    "                    data.append(a_dictionary)\n",
    "                # printing information loop information\n",
    "                loss, acc = model.evaluate(val_df.values, val_target_df.values, verbose=0)\n",
    "                current_loop += 1\n",
    "                print (\"# {} out of {} || Prune frac: {} -- Prune Type: {} -- Accuracy {}\".format(current_loop,\n",
    "                                                                                                total_loops, \n",
    "                                                                                                prune_frac, \n",
    "                                                                                                \"random\" if prune_type == \"random\" else \"influential\", \n",
    "                                                                                                acc))\n",
    "                print(\"Elapsed time:\", datetime.timedelta(seconds=time.time() - start))\n",
    "            \n",
    "    end = datetime.timedelta(seconds=time.time() - start)\n",
    "    df = df.append(data, True)\n",
    "    print(\"Completed {} experiments in {}\".format(total_loops, end))\n",
    "    return df     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training options\n",
    "PRUNE_TYPE = \"influential\"\n",
    "EXPERIMENTS = 5\n",
    "BATCH_SIZE_ARRAY = [50, 100, 125, 150, 175, 200, 225, 250, 300]\n",
    "\n",
    "df = run_dpsgd_scan_batch_size(L2_NORM_CLIP, \n",
    "                               NOISE_MULTIPLIER, \n",
    "                               epoch_scan,\n",
    "                               PRUNE_TYPE,\n",
    "                               prune_indices_list,\n",
    "                               BATCH_SIZE_ARRAY,\n",
    "                               EXPERIMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "df.to_csv('results/dp_sgd_pruning_results_cpave_bs500_double_batch_size_scan.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
